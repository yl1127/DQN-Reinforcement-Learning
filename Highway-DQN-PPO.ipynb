{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE_aM-NDzmCl"
   },
   "source": [
    "\n",
    "# Reinforcement Learning and Self-Driving\n",
    "## AMS 691 Project final report\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This report shows how to use reinforcement learning algorithems(DQN, PPO with attention) to train a agent\n",
    "on the Highway-v0 task from [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv).\n",
    "\n",
    "**Task**\n",
    "In this task, the ego-vehicle agent is driving on a multilane highway populated with other vehicles. The agent's objective is to reach a high speed while avoiding collisions with neighbouring vehicles. Driving on the right side of the road is also rewarded. You can find more\n",
    "information about the environment and other more challenging environments at\n",
    "[Documentation's website](https://highway-env.farama.org/environments/highway/).\n",
    "\n",
    "![highway](Highway-demo.gif)\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "[gymnasium](https://gymnasium.farama.org/) for the environment,\n",
    "installed by using pip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xj7ZG8e6zmCm",
    "outputId": "f4709cb8-085b-45bd-e7bb-9978515e5427"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# !pip install highway-env\n",
    "# !pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq0LyI-vzmCm"
   },
   "source": [
    "**Making an environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uiBxjrIpzmCm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ylpan/anaconda3/envs/yl_RL_Project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "for _ in range(3):\n",
    "    action = env.action_type.actions_indexes[\"IDLE\"]\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-KdnmTIzmCn"
   },
   "source": [
    "**Configuring an environment**\n",
    "\n",
    "The observations, actions, dynamics and rewards of an environment are parametrized by a configuration, defined as a ``config`` dictionary. After environment creation, the configuration can be accessed using the ``config`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I5Q0C-LAzmCn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 40,\n",
      " 'ego_spacing': 2,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 4,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'type': 'Kinematics'},\n",
      " 'offroad_terminal': False,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15,\n",
      " 'vehicles_count': 50,\n",
      " 'vehicles_density': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ylpan/anaconda3/envs/yl_RL_Project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPPsEAYMzmCn"
   },
   "source": [
    "### DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted, cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where $R_{t_0}$ is also known as the *return*. The discount, $\\gamma$, should be a constant between $0$ and $1$ that ensures the sum converges. A lower $\\gamma$ makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about. It also encourages agents to collect reward closer in time than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function $Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tellus what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$. \n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\\end{align}\n",
    "\n",
    "To minimize this error, we will use the Huber loss. The Huber loss acts like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to outliers when the estimates of $Q$ are very noisy. We calculate this over a batch of transitions, $B$, sampled from the replay memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elZJ_khmzmCo"
   },
   "source": [
    "### DQN Training\n",
    "\n",
    "Reinforcement Learning agents can be trained using libraries such as eleurent/rl-agents, openai/baselines or Stable Baselines3. Here is an example of SB3’s DQN implementation trained on highway-fast-v0 with its default kinematics observation and an MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifIlyyJMzmCo"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\")\n",
    "model = DQN('MlpPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=5e-4,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=32,\n",
    "              gamma=0.8,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"highway_dqn/\")\n",
    "model.learn(int(2e4))\n",
    "model.save(\"highway_dqn/model\")\n",
    "\n",
    "# Load and test saved model\n",
    "model = DQN.load(\"highway_dqn/model\")\n",
    "# while True:\n",
    "for _ in range(1):\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "  env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nrliis58zmCo"
   },
   "source": [
    "### DQN Result\n",
    "\n",
    "The following results are obtained:\n",
    "\n",
    "![DQN](yl_DQN_Result.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KddNTXJpzmCp"
   },
   "source": [
    "### PPO algorithm\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a batch of data is being collected and directly consumed to train the policy to maximise the expected return given some proximality constraints. You can think of it as a sophisticated version of statistical gradient-following algorithms, the foundational policy-optimization algorithm. For more information, see the Proximal Policy Optimization Algorithms paper.\n",
    "\n",
    "PPO is usually regarded as a fast and efficient method for online, on-policy reinforcement algorithm. For completeness, here is a brief overview of what the loss computes, the precise formula of the loss is:\n",
    "\n",
    "\\begin{align}L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "    \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "    g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n",
    "    \\right),\\end{align}\n",
    "\n",
    "There are two components in that loss: in the first part of the minimum operator, we simply compute an importance-weighted version of the REINFORCE loss (for example, a REINFORCE loss that we have corrected for the fact that the current policy configuration lags the one that was used for the data collection). The second part of that minimum operator is a similar loss where we have clipped the ratios when they exceeded or were below a given pair of thresholds.\n",
    "\n",
    "This loss ensures that whether the advantage is positive or negative, policy updates that would produce significant shifts from the previous configuration are being discouraged.\n",
    "\n",
    "**Attention**\n",
    "\n",
    "Attention architecture also is considered. Out of a complex scene description, the model should be able to filter information and consider only\n",
    "what is relevant for decision. In other words, the agent should pay attention to vehicles that are close\n",
    "or conflict with the planned route. It is composed of a first linear encoding layer whose weights are shared between all vehicles. It\n",
    "obtains a stochastic matrix called the attention matrix, which is finally used to gather a set of output value $V = [v_0, ..., v_N]$. Overall, the attention computation for each head can be written as:\n",
    "$$output = \\sigma (\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "The outputs from all heads are finally combined with a linear layer, and the resulting tensor is then\n",
    "added to the ego encoding as in residual networks.\n",
    "\n",
    "### PPO Training\n",
    "\n",
    "I include the training code part in Appendix.\n",
    "\n",
    "### PPO Result\n",
    "\n",
    "![PPO](yl_PPO_Attention_Result.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this reinforcement learning project, we applied two distinct methods, Deep Q-Network (DQN) and attention-based Proximal Policy Optimization (PPO), to train an agent in the challenging highway scenarios simulated by the highway-env environment. These methods aimed to create a more adaptable and robust autonomous agent. The attention-based PPO method exhibited notable characteristics, showcasing quick convergence and consistently high mean rewards. This underscored its proficiency to filter information and consider only what is relevant for decision. The agent trained using attention-based PPO exhibited stable behavior. The agent, trained through attention-based PPO, displayed stability in its task.\n",
    "\n",
    "While our results are promising, there remain opportunities for further exploration and enhancement. Future work could focus on multi-agent situations. What we did so far only mimic the human driver and tried to train an agent who had the driving ability as the best human driver. But it's far more enough as we expected. Additionally, investigating a system that can share the environment and reward information may provide deeper insights into the agent's behavior.\n",
    "\n",
    "In conclusion, our reinforcement learning project demonstrated the effectiveness of two methods. The successful training of an adaptive agent in highway scenarios opens avenues for continued research and development in autonomous vehicle navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. Rana A, Malhi A. Building safer autonomous agents by leveraging risky driving behavior knowledge. In2021 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI) 2021 Oct 15 (pp. 1-6). IEEE.\n",
    "\n",
    "2. S. Zhang, L. Wen, H. Peng and H. E. Tseng, \"Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning,\" 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), Indianapolis, IN, USA, 2021, pp. 1745-1752, doi: 10.1109/ITSC48978.2021.9564972.\n",
    "3. Brito B, Agarwal A, Alonso-Mora J. Learning interaction-aware guidance policies for motion planning in dense traffic scenarios. arXiv preprint arXiv:2107.04538. 2021 Jul 9.\n",
    "\n",
    "4. X. Chen, J. Wei, X. Ren, K. H. Johansson and X. Wang, \"Automatic Overtaking on Two-way Roads with Vehicle Interactions Based on Proximal Policy Optimization,\" 2021 IEEE Intelligent Vehicles Symposium (IV), Nagoya, Japan, 2021, pp. 1057-1064, doi: 10.1109/IV48863.2021.9575954.\n",
    "\n",
    "5. Schott L, Hajri H, Lamprier S. Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network. In2022 International Joint Conference on Neural Networks (IJCNN) 2022 Jul 18 (pp. 1-8). IEEE.\n",
    "\n",
    "6. Liu Q, Dang F, Wang X, Ren X. Autonomous highway merging in mixed traffic using reinforcement learning and motion predictive safety controller. In2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC) 2022 Oct 8 (pp. 1063-1069). IEEE.\n",
    "\n",
    "7. Leurent E. Safe and Efficient Reinforcement Learning for Behavioural Planning in Autonomous Driving In2020 \n",
    "\n",
    "8. Lazzaroni, L., Bellotti, F., Capello, A., Cossu, M., De Gloria, A., Berta, R. (2023). Deep Reinforcement Learning for Automated Car Parking. In: Berta, R., De Gloria, A. (eds) Applications in Electronics Pervading Industry, Environment and Society. ApplePies 2022. Lecture Notes in Electrical Engineering, vol 1036. Springer, Cham. https://doi.org/10.1007/978-3-031-30333-3_16\n",
    "\n",
    "9. Leurent E, Mercat J. Social attention for autonomous decision-making in dense traffic. arXiv preprint arXiv:1911.12250. 2019 Nov 27.\n",
    "\n",
    "10. Zhang, H., Chen, W., Huang, Z., Li, M., Yang, Y., Zhang, W., & Wang, J. (2020). Bi-Level Actor-Critic for Multi-Agent Coordination. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 7325-7332. https://doi.org/10.1609/aaai.v34i05.6226\n",
    "\n",
    "11. Sriram NN, Liu B, Pittaluga F, Chandraker M. Smart: Simultaneous multi-agent recurrent trajectory prediction. InComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16 2020 (pp. 463-479). Springer International Publishing.\n",
    "\n",
    "12. Chen B, Xu M, Liu Z, Li L, Zhao D. Delay-aware multi-agent reinforcement learning for cooperative and competitive environments. arXiv preprint arXiv:2005.05441. 2020 May 11.\n",
    "\n",
    "13. Williams, R.J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach Learn 8, 229–256 (1992). https://doi.org/10.1007/BF00992696\n",
    "\n",
    "14. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 2017 Jul 20.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
